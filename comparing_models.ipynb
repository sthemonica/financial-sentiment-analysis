{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['output','text']\n",
    "df = pd.read_csv('all-data.csv', names=colnames, encoding = \"ISO-8859-1\", header=None)\n",
    "all_sentences = [text for text in df.text]\n",
    "words = ' '.join(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "token_punct = tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to transform and clean list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformSentence(list_of_sentences):\n",
    "    \n",
    "    sentences_after_stopwords = list()\n",
    "    \n",
    "    for sentence in list_of_sentences:\n",
    "        new_sentence = list()\n",
    "        words_from_sentence = token_space.tokenize(sentence)\n",
    "        for word in words_from_sentence:\n",
    "            if word not in stop_words:\n",
    "                new_sentence.append(word)\n",
    "        sentences_after_stopwords.append(\" \".join(new_sentence))\n",
    "\n",
    "    sentences_after_stopwords_puncts = list()\n",
    "\n",
    "    for sentence in sentences_after_stopwords:\n",
    "        for punct_to_change in punctuation:\n",
    "            sentence = sentence.replace(punct_to_change,\"\")\n",
    "        sentences_after_stopwords_puncts.append(sentence)\n",
    "\n",
    "    sentences_after_stopwords_puncts_lower = list()\n",
    "\n",
    "    for sentence in sentences_after_stopwords_puncts:\n",
    "        sentence = sentence.lower()\n",
    "        sentences_after_stopwords_puncts_lower.append(sentence)\n",
    "\n",
    "    stop_words_no_accent = list()\n",
    "\n",
    "    for word in stop_words:\n",
    "        for punct_to_change in punctuation:\n",
    "            word = word.replace(punct_to_change,\"\")\n",
    "        stop_words_no_accent.append(word)\n",
    "    \n",
    "    sentences_after_stopwords_puncts_lower_stopwords = list()\n",
    "\n",
    "    for sentence in sentences_after_stopwords_puncts_lower:\n",
    "        new_sentence = list()\n",
    "        words_from_sentence = token_space.tokenize(sentence)\n",
    "        for word in words_from_sentence:\n",
    "            if word not in stop_words_no_accent:\n",
    "                new_sentence.append(word)\n",
    "        sentences_after_stopwords_puncts_lower_stopwords.append(\" \".join(new_sentence))\n",
    "\n",
    "    sentences_after_stopwords_puncts_lower_stopwords_number = list()\n",
    "\n",
    "    for sentence in sentences_after_stopwords_puncts_lower_stopwords:\n",
    "        new_sentence = list()\n",
    "        words_from_sentence = token_space.tokenize(sentence)\n",
    "        for word in words_from_sentence:\n",
    "            if not word.isnumeric():\n",
    "                new_sentence.append(word)\n",
    "            else:\n",
    "                new_sentence.append(\"0\")\n",
    "        sentences_after_stopwords_puncts_lower_stopwords_number.append(\" \".join(new_sentence))\n",
    "\n",
    "    return sentences_after_stopwords_puncts_lower_stopwords_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_sentences = transformSentence(list(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the imbalance between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4846, 600)\n",
      "(4846,)\n",
      "neutral     2879\n",
      "positive    1363\n",
      "negative     604\n",
      "Name: output, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False,max_features=600)\n",
    "vector_tfidf = tfidf.fit_transform(treated_sentences)\n",
    "print(vector_tfidf.shape)\n",
    "print(df.output.shape)\n",
    "print(df.output.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8637, 600)\n",
      "(8637,)\n",
      "neutral     2879\n",
      "negative    2879\n",
      "positive    2879\n",
      "Name: output, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=100)\n",
    "X_resampled, Y_resampled = smote.fit_resample(vector_tfidf,df.output)\n",
    "print(X_resampled.shape)\n",
    "print(Y_resampled.shape)\n",
    "print(Y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting in Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.1\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_resampled,Y_resampled,random_state = 100,test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (7773, 600)\n",
      "test: (864, 600)\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\",X_train.shape)\n",
    "print(\"test:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615740740740741"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(solver = 'lbfgs',penalty = 'l2',max_iter = 5000)\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7800925925925926"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(solver = 'lbfgs',penalty = 'none',max_iter = 5000)\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615740740740741"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(solver = 'sag',penalty = 'l2',max_iter = 5000)\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7766203703703703"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(solver = 'sag',penalty = 'none',max_iter = 5000)\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6712962962962963"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes = MultinomialNB(alpha=1)\n",
    "naive_bayes.fit(X_train,Y_train)\n",
    "naive_bayes.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8576388888888888"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = RandomForestClassifier(random_state= 100)\n",
    "randomForest.fit(X_train,Y_train)\n",
    "randomForest.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"criterion\": ['gini','entropy'],\n",
    "#     \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "#     \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "# }\n",
    "\n",
    "# randomForest = RandomForestClassifier(n_estimators = 300,max_depth=None, random_state=100)\n",
    "# random_cv = RandomizedSearchCV(randomForest, param_grid, n_iter=20, cv=5, n_jobs=-1, random_state = 100)\n",
    "# rcv = random_cv.fit(X_train, Y_train)\n",
    "# rcv.best_params_\n",
    "\n",
    "# randomForest = RandomForestClassifier(**rcv.best_params_, random_state = 100)\n",
    "# randomForest.fit(X_train,Y_train)\n",
    "# randomForest.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8761574074074074"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = RandomForestClassifier(n_estimators = 300,max_depth=None, min_samples_leaf = 1, max_features = 'log2', criterion = 'entropy', random_state = 100)\n",
    "randomForest.fit(X_train,Y_train)\n",
    "randomForest.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7731481481481481"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisionTree = DecisionTreeClassifier(random_state = 100)\n",
    "decisionTree.fit(X_train,Y_train)\n",
    "decisionTree.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'splitter': 'random',\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"criterion\": ['gini','entropy'],\n",
    "    \"splitter\": [\"best\",\"random\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(random_state = 100)\n",
    "random_cv = RandomizedSearchCV(decisionTree, param_grid, n_iter=20, cv=5, n_jobs=-1, random_state = 100)\n",
    "rcv = random_cv.fit(X_train, Y_train)\n",
    "rcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7662037037037037"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisionTree = DecisionTreeClassifier(**rcv.best_params_,random_state = 100)\n",
    "decisionTree.fit(X_train,Y_train)\n",
    "decisionTree.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8657407407407407"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_vector = svm.SVC(random_state = 100)\n",
    "support_vector.fit(X_train,Y_train)\n",
    "support_vector.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'rbf', 'gamma': 'scale', 'degree': 9}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"kernel\": ['linear','poly','rbf','sigmoid'],\n",
    "    \"gamma\": ['scale','auto'],\n",
    "    \"degree\": [3,5,7,9]\n",
    "}\n",
    "\n",
    "support_vector = svm.SVC(random_state = 100)\n",
    "random_cv = RandomizedSearchCV(support_vector, param_grid, n_iter=20, cv=5, n_jobs=-1, random_state = 100)\n",
    "rcv = random_cv.fit(X_train, Y_train)\n",
    "rcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8657407407407407"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "support_vector = svm.SVC(**rcv.best_params_,random_state = 100)\n",
    "support_vector.fit(X_train,Y_train)\n",
    "support_vector.score(X_test,Y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "051f523ea69bc1770ecd2306c10409abac68aa2062faba780e671356775dd235"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
