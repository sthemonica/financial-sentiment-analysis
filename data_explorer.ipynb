{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['output','text']\n",
    "df = pd.read_csv('all-data.csv', names=colnames, encoding = \"ISO-8859-1\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download our dataset. Data extracted from: https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     output                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_output = df.output.unique()\n",
    "list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     2879\n",
       "positive    1363\n",
       "negative     604\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.output.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of outputs:\n",
      "neutral: 0.594098225340487\n",
      "positive: 0.28126289723483283\n",
      "negative: 0.12463887742468015\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of outputs:')\n",
    "print('neutral:',df.output.value_counts()['neutral']/df.output.count())\n",
    "print('positive:',df.output.value_counts()['positive']/df.output.count())\n",
    "print('negative:',df.output.value_counts()['negative']/df.output.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, there is an imbalance between all three outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 4780, 'the': 4712, ',': 4662, 'of': 3202, 'in': 2752, 'and': 2584, 'to': 2496, 'a': 1631, 'The': 1354, 'for': 1129, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "token_words = token_space.tokenize(words)\n",
    "frequency = nltk.FreqDist(token_words)\n",
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "token_punct = tokenize.WordPunctTokenizer()\n",
    "puncts = list()\n",
    "for punct in punctuation:\n",
    "    puncts.append(punct)\n",
    "punct_and_stopwords = puncts + stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_after_stopwords = list()\n",
    "\n",
    "for sentence in df.text:\n",
    "    new_sentence = list()\n",
    "    words_from_sentence = token_space.tokenize(sentence)\n",
    "    for word in words_from_sentence:\n",
    "        if word not in stop_words:\n",
    "            new_sentence.append(word)\n",
    "    sentences_after_stopwords.append(\" \".join(new_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove puncts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_after_stopwords_puncts = list()\n",
    "\n",
    "for sentence in sentences_after_stopwords:\n",
    "    new_sentence = list()\n",
    "    words_from_sentence = token_punct.tokenize(sentence)\n",
    "    for word in words_from_sentence:\n",
    "        if word not in punct_and_stopwords:\n",
    "            new_sentence.append(word)\n",
    "    sentences_after_stopwords_puncts.append(\" \".join(new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_no_accent = [unidecode.unidecode(text) for text in sentences_after_stopwords_puncts]\n",
    "stop_words_no_accent = [unidecode.unidecode(text) for text in punct_and_stopwords]\n",
    "\n",
    "sentences_after_stopwords_puncts_accents = list()\n",
    "\n",
    "for sentence in words_with_no_accent:\n",
    "    new_sentence = list()\n",
    "    words_from_sentence = token_punct.tokenize(sentence)\n",
    "    for word in words_from_sentence:\n",
    "        if word not in stop_words_no_accent:\n",
    "            new_sentence.append(word)\n",
    "    sentences_after_stopwords_puncts_accents.append(\" \".join(new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_after_stopwords_puncts_accents_lower = list()\n",
    "for sentence in sentences_after_stopwords_puncts_accents:\n",
    "    lower_sentence = sentence.lower()\n",
    "    sentences_after_stopwords_puncts_accents_lower.append(\"\".join(lower_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False,max_features=600)\n",
    "vector_tfidf = tfidf.fit_transform(sentences_after_stopwords_puncts_accents_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(vector_tfidf.todense(),columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(vector_tfidf,df.output,random_state = 100,test_size=0.1)\n",
    "logistic_regression = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734020618556701"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "051f523ea69bc1770ecd2306c10409abac68aa2062faba780e671356775dd235"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
